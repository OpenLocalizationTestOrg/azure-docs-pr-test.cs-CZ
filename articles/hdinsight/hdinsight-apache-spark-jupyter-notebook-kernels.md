---
title: "Jádra pro poznámkový blok Jupyter na clustery Spark v Azure HDInsight | Microsoft Docs"
description: "Další informace o jádra PySpark, PySpark3 a Spark pro poznámkový blok Jupyter s clustery Spark v Azure HDInsight k dispozici."
keywords: "Poznámkový blok jupyter na spark, jupyter spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: cs-CZ
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="177a9-104">Jádra pro poznámkový blok Jupyter na clustery Spark v Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="177a9-105">Clustery HDInsight Spark poskytují jádra, která můžete použít s poznámkovým blokem Jupyter na Spark pro testování vašich aplikací.</span><span class="sxs-lookup"><span data-stu-id="177a9-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="177a9-106">Jádro je program, který spouští a interpretuje vašeho kódu.</span><span class="sxs-lookup"><span data-stu-id="177a9-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="177a9-107">Jsou tři jádra:</span><span class="sxs-lookup"><span data-stu-id="177a9-107">The three kernels are:</span></span>

- <span data-ttu-id="177a9-108">**PySpark** – pro aplikace napsané v Python2</span><span class="sxs-lookup"><span data-stu-id="177a9-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="177a9-109">**PySpark3** – pro aplikace napsané v Python3</span><span class="sxs-lookup"><span data-stu-id="177a9-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="177a9-110">**Spark** – pro aplikace napsané v jazyce Scala</span><span class="sxs-lookup"><span data-stu-id="177a9-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="177a9-111">V tomto článku můžete další informace o použití těchto jádra a výhody jejich používání.</span><span class="sxs-lookup"><span data-stu-id="177a9-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="177a9-112">Požadavky</span><span class="sxs-lookup"><span data-stu-id="177a9-112">Prerequisites</span></span>

* <span data-ttu-id="177a9-113">Cluster Apache Spark v HDInsight.</span><span class="sxs-lookup"><span data-stu-id="177a9-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="177a9-114">Pokyny najdete v tématu [clusterů vytvořit Apache Spark v Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="177a9-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="177a9-115">Vytvoření poznámkového bloku Jupyter v Spark HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="177a9-116">Z [portál Azure](https://portal.azure.com/), otevřete váš cluster.</span><span class="sxs-lookup"><span data-stu-id="177a9-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="177a9-117">V tématu [seznamu a zobrazit clustery](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) pokyny.</span><span class="sxs-lookup"><span data-stu-id="177a9-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="177a9-118">Cluster se otevře v novém okně portálu.</span><span class="sxs-lookup"><span data-stu-id="177a9-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="177a9-119">Z **rychlé odkazy** klikněte na tlačítko **clusteru řídicí panely** otevřete **clusteru řídicí panely** okno.</span><span class="sxs-lookup"><span data-stu-id="177a9-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="177a9-120">Pokud nevidíte **rychlé odkazy**, klikněte na tlačítko **přehled** v levé nabídce v okně.</span><span class="sxs-lookup"><span data-stu-id="177a9-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="177a9-121">![Poznámkový blok Jupyter na Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Poznámkový blok Jupyter na Spark")</span><span class="sxs-lookup"><span data-stu-id="177a9-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="177a9-122">Klikněte na tlačítko **Poznámkový blok Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="177a9-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="177a9-123">Po vyzvání zadejte přihlašovací údaje správce clusteru.</span><span class="sxs-lookup"><span data-stu-id="177a9-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="177a9-124">Otevření následující adresy URL v prohlížeči, mohou také spojit poznámkového bloku Jupyter v clusteru Spark.</span><span class="sxs-lookup"><span data-stu-id="177a9-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="177a9-125">Nahraďte **CLUSTERNAME** názvem clusteru:</span><span class="sxs-lookup"><span data-stu-id="177a9-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="177a9-126">Klikněte na tlačítko **nový**a pak klikněte buď **Pyspark**, **PySpark3**, nebo **Spark** k vytvoření poznámkového bloku.</span><span class="sxs-lookup"><span data-stu-id="177a9-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="177a9-127">Použijte jádra Spark Scala aplikací, jádra PySpark pro Python2 aplikace a PySpark3 jádra pro Python3 aplikace.</span><span class="sxs-lookup"><span data-stu-id="177a9-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="177a9-128">![Jádra pro poznámkový blok Jupyter na Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "jádra pro poznámkový blok Jupyter na Spark")</span><span class="sxs-lookup"><span data-stu-id="177a9-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="177a9-129">Poznámkový blok se otevře s jádrem, které jste vybrali.</span><span class="sxs-lookup"><span data-stu-id="177a9-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="177a9-130">Výhody použití jader</span><span class="sxs-lookup"><span data-stu-id="177a9-130">Benefits of using the kernels</span></span>

<span data-ttu-id="177a9-131">Zde naleznete několik výhod nového jádrech pomocí poznámkového bloku Jupyter na clustery Spark HDInsight.</span><span class="sxs-lookup"><span data-stu-id="177a9-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="177a9-132">**Předvolby kontexty**.</span><span class="sxs-lookup"><span data-stu-id="177a9-132">**Preset contexts**.</span></span> <span data-ttu-id="177a9-133">S **PySpark**, **PySpark3**, nebo **Spark** jádra, není nutné explicitně nastavovat kontexty Spark nebo Hive před zahájením práce s vašimi aplikacemi.</span><span class="sxs-lookup"><span data-stu-id="177a9-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="177a9-134">Toto jsou k dispozici ve výchozím nastavení.</span><span class="sxs-lookup"><span data-stu-id="177a9-134">These are available by default.</span></span> <span data-ttu-id="177a9-135">Tyto kontexty jsou:</span><span class="sxs-lookup"><span data-stu-id="177a9-135">These contexts are:</span></span>
   
   * <span data-ttu-id="177a9-136">**sc** – pro kontext Spark</span><span class="sxs-lookup"><span data-stu-id="177a9-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="177a9-137">**sqlContext** – pro kontext Hive</span><span class="sxs-lookup"><span data-stu-id="177a9-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="177a9-138">Ano nemusíte spouštět příkazy jako je třeba následující příkaz a nastavovat kontexty:</span><span class="sxs-lookup"><span data-stu-id="177a9-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="177a9-139">sc = SparkContext('yarn-client') sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="177a9-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="177a9-140">Místo toho můžete přímo použít přednastavení kontexty ve vaší aplikaci.</span><span class="sxs-lookup"><span data-stu-id="177a9-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="177a9-141">**Buňky Magic**.</span><span class="sxs-lookup"><span data-stu-id="177a9-141">**Cell magics**.</span></span> <span data-ttu-id="177a9-142">Poskytuje jádra PySpark některé předdefinované "Magic", které jsou speciální příkazy, které můžete volat s `%%` (například `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="177a9-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="177a9-143">Příkaz magic musí být první slovo v buňce kódu a povolit pro více řádků obsahu.</span><span class="sxs-lookup"><span data-stu-id="177a9-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="177a9-144">Magic slovo by měl být první slovo v buňce.</span><span class="sxs-lookup"><span data-stu-id="177a9-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="177a9-145">Přidání nic před magic, i komentáře, výsledkem bude chyba.</span><span class="sxs-lookup"><span data-stu-id="177a9-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="177a9-146">Další informace o Magic, které najdete v tématu [zde](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="177a9-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="177a9-147">Následující tabulka uvádí různé Magic, které jsou k dispozici prostřednictvím jádrech.</span><span class="sxs-lookup"><span data-stu-id="177a9-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="177a9-148">Magic</span><span class="sxs-lookup"><span data-stu-id="177a9-148">Magic</span></span> | <span data-ttu-id="177a9-149">Příklad</span><span class="sxs-lookup"><span data-stu-id="177a9-149">Example</span></span> | <span data-ttu-id="177a9-150">Popis</span><span class="sxs-lookup"><span data-stu-id="177a9-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="177a9-151">Pomoc</span><span class="sxs-lookup"><span data-stu-id="177a9-151">help</span></span> |`%%help` |<span data-ttu-id="177a9-152">Vytvoří tabulku všechny dostupné Magic s příklad a popis</span><span class="sxs-lookup"><span data-stu-id="177a9-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="177a9-153">Informace o</span><span class="sxs-lookup"><span data-stu-id="177a9-153">info</span></span> |`%%info` |<span data-ttu-id="177a9-154">Výstupy informací o relaci pro aktuální koncový bod Livy</span><span class="sxs-lookup"><span data-stu-id="177a9-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="177a9-155">Konfigurace</span><span class="sxs-lookup"><span data-stu-id="177a9-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="177a9-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="177a9-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="177a9-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="177a9-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="177a9-158">Nakonfiguruje parametry pro vytvoření relace.</span><span class="sxs-lookup"><span data-stu-id="177a9-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="177a9-159">Příznak force (-f) je povinný, pokud je relace již bylo vytvořeno, který zajistí, že je relace vyřadit a vytvořit znovu.</span><span class="sxs-lookup"><span data-stu-id="177a9-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="177a9-160">Podívejte se na [/sessions POST na Livy text žádosti](https://github.com/cloudera/livy#request-body) pro seznam platných parametrů.</span><span class="sxs-lookup"><span data-stu-id="177a9-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="177a9-161">Parametry musí být předán jako řetězec formátu JSON a musí být na další řádek po magic, jak je znázorněno v příkladu sloupec.</span><span class="sxs-lookup"><span data-stu-id="177a9-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="177a9-162">SQL</span><span class="sxs-lookup"><span data-stu-id="177a9-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="177a9-163">Provede dotaz Hive proti sqlContext.</span><span class="sxs-lookup"><span data-stu-id="177a9-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="177a9-164">Pokud `-o` parametr se předává, výsledek dotazu je uchován v %% lokální kontext Python jako [Pandas](http://pandas.pydata.org/) dataframe.</span><span class="sxs-lookup"><span data-stu-id="177a9-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="177a9-165">místní</span><span class="sxs-lookup"><span data-stu-id="177a9-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="177a9-166">Všechny kód na další řádek je spustit místně.</span><span class="sxs-lookup"><span data-stu-id="177a9-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="177a9-167">Kód musí být platný kód Python2 i bez ohledu na jádra, který používáte.</span><span class="sxs-lookup"><span data-stu-id="177a9-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="177a9-168">Ano, i v případě, že jste vybrali **PySpark3** nebo **Spark** jádra při vytváření poznámkového bloku, pokud použijete `%%local` magic v buňce, dané buňky musí mít pouze platný kód Python2...</span><span class="sxs-lookup"><span data-stu-id="177a9-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="177a9-169">Protokoly</span><span class="sxs-lookup"><span data-stu-id="177a9-169">logs</span></span> |`%%logs` |<span data-ttu-id="177a9-170">Protokoly pro aktuální relaci Livy výstupy.</span><span class="sxs-lookup"><span data-stu-id="177a9-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="177a9-171">Odstranit</span><span class="sxs-lookup"><span data-stu-id="177a9-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="177a9-172">Odstraní relaci konkrétní aktuální Livy koncového bodu.</span><span class="sxs-lookup"><span data-stu-id="177a9-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="177a9-173">Poznámka: nelze odstranit iniciovaného relace pro jádra sám sebe.</span><span class="sxs-lookup"><span data-stu-id="177a9-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="177a9-174">Čištění</span><span class="sxs-lookup"><span data-stu-id="177a9-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="177a9-175">Odstraní všechny relace pro aktuální Livy koncový bod, včetně relace tento poznámkový blok.</span><span class="sxs-lookup"><span data-stu-id="177a9-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="177a9-176">Příznak force -f je povinný.</span><span class="sxs-lookup"><span data-stu-id="177a9-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="177a9-177">Kromě Magic přidal jádra PySpark, můžete také použít [předdefinované Magic IPython](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), včetně `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="177a9-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="177a9-178">Můžete použít `%%sh` magic spouštět skripty a blok kódu na headnode clusteru.</span><span class="sxs-lookup"><span data-stu-id="177a9-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="177a9-179">**Automaticky vizualizace**.</span><span class="sxs-lookup"><span data-stu-id="177a9-179">**Auto visualization**.</span></span> <span data-ttu-id="177a9-180">**Pyspark** jádra automaticky vizualizuje výstup dotazy Hive a SQL.</span><span class="sxs-lookup"><span data-stu-id="177a9-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="177a9-181">Můžete zvolit několika různých typů vizualizace včetně tabulky, kruhový, řádku, oblasti, panelu.</span><span class="sxs-lookup"><span data-stu-id="177a9-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="177a9-182">Parametry podporovány s %% sql magic</span><span class="sxs-lookup"><span data-stu-id="177a9-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="177a9-183">`%%sql` Magic podporuje různé parametry, které můžete použít k řízení druh výstup, která se zobrazí při spuštění dotazů.</span><span class="sxs-lookup"><span data-stu-id="177a9-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="177a9-184">Následující tabulka uvádí výstup.</span><span class="sxs-lookup"><span data-stu-id="177a9-184">The following table lists the output.</span></span>

| <span data-ttu-id="177a9-185">Parametr</span><span class="sxs-lookup"><span data-stu-id="177a9-185">Parameter</span></span> | <span data-ttu-id="177a9-186">Příklad</span><span class="sxs-lookup"><span data-stu-id="177a9-186">Example</span></span> | <span data-ttu-id="177a9-187">Popis</span><span class="sxs-lookup"><span data-stu-id="177a9-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="177a9-188">-o</span><span class="sxs-lookup"><span data-stu-id="177a9-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="177a9-189">Tento parametr použijte pro uchování v výsledek dotazu, %% lokální kontext Python, jako [Pandas](http://pandas.pydata.org/) dataframe.</span><span class="sxs-lookup"><span data-stu-id="177a9-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="177a9-190">Název proměnné dataframe je název proměnné, které zadáte.</span><span class="sxs-lookup"><span data-stu-id="177a9-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="177a9-191">-q</span><span class="sxs-lookup"><span data-stu-id="177a9-191">-q</span></span> |`-q` |<span data-ttu-id="177a9-192">Použijte k vypnutí možnosti vizualizace pro buňky.</span><span class="sxs-lookup"><span data-stu-id="177a9-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="177a9-193">Pokud nechcete automaticky vizualizovat obsah buňky a chcete jen zaznamenat jako dataframe, potom použijte `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="177a9-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="177a9-194">Pokud chcete vypnout vizualizace bez zaznamenávání výsledky (například pro spuštění příkazu jazyka SQL, jako je třeba `CREATE TABLE` příkaz), použijte `-q` bez zadání `-o` argument.</span><span class="sxs-lookup"><span data-stu-id="177a9-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="177a9-195">-m</span><span class="sxs-lookup"><span data-stu-id="177a9-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="177a9-196">Kde **metoda** je buď **trvat** nebo **ukázka** (výchozí hodnota je **trvat**).</span><span class="sxs-lookup"><span data-stu-id="177a9-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="177a9-197">Pokud je metoda **trvat**, jádra vybere elementy z horní části datové sady výsledků dotazu určeného MAXROWS (popsané dál v této tabulce).</span><span class="sxs-lookup"><span data-stu-id="177a9-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="177a9-198">Pokud je metoda **ukázka**, jádra náhodně ukázky elementy sady dat podle `-r` parametr popsána dále v této tabulce.</span><span class="sxs-lookup"><span data-stu-id="177a9-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="177a9-199">-r</span><span class="sxs-lookup"><span data-stu-id="177a9-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="177a9-200">Zde **ZLOMEK** je číslo s plovoucí desetinnou čárkou mezi 0,0 a 1,0.</span><span class="sxs-lookup"><span data-stu-id="177a9-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="177a9-201">Pokud je metoda ukázka pro příkaz jazyka SQL `sample`, pak jádra náhodně ukázky zadaný podíl elementy výsledku nastavení za vás.</span><span class="sxs-lookup"><span data-stu-id="177a9-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="177a9-202">Například pokud spustíte dotaz SQL s argumenty `-m sample -r 0.01`, pak se náhodně vzorkovat 1 % řádků výsledek.</span><span class="sxs-lookup"><span data-stu-id="177a9-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="177a9-203">**MAXROWS** celočíselná hodnota.</span><span class="sxs-lookup"><span data-stu-id="177a9-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="177a9-204">Jádra omezuje počet řádků výstup do **MAXROWS**.</span><span class="sxs-lookup"><span data-stu-id="177a9-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="177a9-205">Pokud **MAXROWS** záporné číslo, jako je **-1**, pak není omezený počet řádků v sadě výsledků dotazu.</span><span class="sxs-lookup"><span data-stu-id="177a9-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="177a9-206">**Příklad:**</span><span class="sxs-lookup"><span data-stu-id="177a9-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="177a9-207">Výše uvedený příkaz provede následující akce:</span><span class="sxs-lookup"><span data-stu-id="177a9-207">The statement above does the following:</span></span>

* <span data-ttu-id="177a9-208">Vybere všechny záznamy z **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="177a9-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="177a9-209">Vzhledem k tomu, že používáme - q, vypne automatické vizualizace.</span><span class="sxs-lookup"><span data-stu-id="177a9-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="177a9-210">Vzhledem k tomu, že používáme `-m sample -r 0.1 -n 500` náhodně ukázky 10 % řádků v hivesampletable a omezení velikosti sady výsledků do 500 řádků.</span><span class="sxs-lookup"><span data-stu-id="177a9-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="177a9-211">Nakonec protože jsme použili `-o query2` také uloží výstup do dataframe, nazývá **dotaz2**.</span><span class="sxs-lookup"><span data-stu-id="177a9-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="177a9-212">Aspekty při použití nové jádra</span><span class="sxs-lookup"><span data-stu-id="177a9-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="177a9-213">Podle toho, která jádra, které používáte, ponechat poznámkových bloků systémem spotřebovává prostředky clusteru.</span><span class="sxs-lookup"><span data-stu-id="177a9-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="177a9-214">S těmito jádra protože kontexty jsou předvolby, jednoduše ukončení poznámkových bloků kill není kontextu a proto nadále používat prostředky clusteru.</span><span class="sxs-lookup"><span data-stu-id="177a9-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="177a9-215">Je vhodné použít **zavřít a zastavit** možnost poznámkového bloku **souboru** nabídky po skončení pomocí poznámkového bloku, který ukončí kontext a poté ukončí poznámkového bloku.</span><span class="sxs-lookup"><span data-stu-id="177a9-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="177a9-216">Ukázat některé příklady</span><span class="sxs-lookup"><span data-stu-id="177a9-216">Show me some examples</span></span>

<span data-ttu-id="177a9-217">Otevřete Poznámkový blok Jupyter, uvidíte dvě složky k dispozici na kořenové úrovni.</span><span class="sxs-lookup"><span data-stu-id="177a9-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="177a9-218">**PySpark** složka obsahuje ukázkové poznámkových bloků, které používají nové **Python** jádra.</span><span class="sxs-lookup"><span data-stu-id="177a9-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="177a9-219">**Scala** složka obsahuje ukázkové poznámkových bloků, které používají nové **Spark** jádra.</span><span class="sxs-lookup"><span data-stu-id="177a9-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="177a9-220">Můžete otevřít **00 - [přečtěte si NEJPRVE] funkce jádra Magic Spark** poznámkového bloku z **PySpark** nebo **Spark** složku pro další informace o různých Magic, které jsou k dispozici.</span><span class="sxs-lookup"><span data-stu-id="177a9-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="177a9-221">Můžete taky dostupná ukázková notebooky pod dvě složky se dozvíte, jak zajistit různé scénáře použití poznámkové bloky Jupyter s clustery HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="177a9-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="177a9-222">Kde jsou uložené poznámkových bloků?</span><span class="sxs-lookup"><span data-stu-id="177a9-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="177a9-223">Poznámkové bloky Jupyter se uloží do účtu úložiště přidruženého k clusteru pod **/HdiNotebooks** složky.</span><span class="sxs-lookup"><span data-stu-id="177a9-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="177a9-224">Poznámkové bloky, textové soubory a složky, které vytvoříte z v rámci Jupyter jsou přístupné z účtu úložiště.</span><span class="sxs-lookup"><span data-stu-id="177a9-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="177a9-225">Například, pokud používáte Jupyter vytvořit složku **Moje_složka** a Poznámkový blok **myfolder/mynotebook.ipynb**, dostanete tento poznámkový blok v `/HdiNotebooks/myfolder/mynotebook.ipynb` v rámci účtu úložiště.</span><span class="sxs-lookup"><span data-stu-id="177a9-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="177a9-226">Platí také nastavena hodnota true, to znamená, pokud nahrát Poznámkový blok přímo do účtu úložiště na `/HdiNotebooks/mynotebook1.ipynb`, Poznámkový blok je také zobrazit z Jupyter.</span><span class="sxs-lookup"><span data-stu-id="177a9-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="177a9-227">Poznámkové bloky zůstat v účtu úložiště i po odstranění clusteru.</span><span class="sxs-lookup"><span data-stu-id="177a9-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="177a9-228">Způsob, jakým poznámkových bloků se uloží do účtu úložiště je kompatibilní s HDFS.</span><span class="sxs-lookup"><span data-stu-id="177a9-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="177a9-229">Pokud tedy můžete SSH do clusteru, který můžete použít soubor příkazy pro správu, jak je znázorněno v následujícím fragmentu kódu:</span><span class="sxs-lookup"><span data-stu-id="177a9-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="177a9-230">V případě, že nedochází k potížím přístupu k účtu úložiště pro cluster, poznámkových bloků jsou také uloženy na headnode `/var/lib/jupyter`.</span><span class="sxs-lookup"><span data-stu-id="177a9-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="177a9-231">Podporovaný prohlížeč</span><span class="sxs-lookup"><span data-stu-id="177a9-231">Supported browser</span></span>

<span data-ttu-id="177a9-232">Poznámkové bloky Jupyter na clustery Spark HDInsight jsou podporovány pouze na Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="177a9-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="177a9-233">Váš názor</span><span class="sxs-lookup"><span data-stu-id="177a9-233">Feedback</span></span>
<span data-ttu-id="177a9-234">Nové jádrech jsou v vyvíjející se fáze a bude pro dospělé v čase.</span><span class="sxs-lookup"><span data-stu-id="177a9-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="177a9-235">To může znamenat, že rozhraní API může změnit, protože tyto jádra pro dospělé.</span><span class="sxs-lookup"><span data-stu-id="177a9-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="177a9-236">Uvítáme jakékoli zpětnou vazbu, která máte při použití těchto nových jádra.</span><span class="sxs-lookup"><span data-stu-id="177a9-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="177a9-237">To je užitečné v shaping finální verzi nástroje tyto jádra.</span><span class="sxs-lookup"><span data-stu-id="177a9-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="177a9-238">Můžete ponechat vaše komentáře nebo zpětné vazby v části **komentáře** v dolní části tohoto článku.</span><span class="sxs-lookup"><span data-stu-id="177a9-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="177a9-239"><a name="seealso"></a>Viz také</span><span class="sxs-lookup"><span data-stu-id="177a9-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="177a9-240">Přehled: Apache Spark v Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="177a9-241">Scénáře</span><span class="sxs-lookup"><span data-stu-id="177a9-241">Scenarios</span></span>
* [<span data-ttu-id="177a9-242">Spark s BI: Provádějte interaktivní analýzy dat pomocí Sparku v HDInsight pomocí nástrojů BI</span><span class="sxs-lookup"><span data-stu-id="177a9-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="177a9-243">Spark s Machine Learning: Používejte Spark v HDInsight pro analýzu teploty v budově pomocí dat HVAC</span><span class="sxs-lookup"><span data-stu-id="177a9-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="177a9-244">Spark s Machine Learning: Používejte Spark v HDInsight k předpovědím výsledků kontrol potravin</span><span class="sxs-lookup"><span data-stu-id="177a9-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="177a9-245">Datové proudy Spark: Používejte Spark v HDInsight pro sestavení aplikací datových proudů v reálném čase</span><span class="sxs-lookup"><span data-stu-id="177a9-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="177a9-246">Analýza protokolu webu pomocí Sparku v HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="177a9-247">Vytvoření a spouštění aplikací</span><span class="sxs-lookup"><span data-stu-id="177a9-247">Create and run applications</span></span>
* [<span data-ttu-id="177a9-248">Vytvoření samostatné aplikace pomocí Scala</span><span class="sxs-lookup"><span data-stu-id="177a9-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="177a9-249">Vzdálené spouštění úloh na clusteru Sparku pomocí Livy</span><span class="sxs-lookup"><span data-stu-id="177a9-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="177a9-250">Nástroje a rozšíření</span><span class="sxs-lookup"><span data-stu-id="177a9-250">Tools and extensions</span></span>
* [<span data-ttu-id="177a9-251">Modul plug-in nástroje HDInsight pro IntelliJ IDEA pro vytvoření a odesílání aplikací Spark Scala</span><span class="sxs-lookup"><span data-stu-id="177a9-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="177a9-252">Použití modulu plug-in nástroje HDInsight pro IntelliJ IDEA pro vzdálené ladění aplikací Spark</span><span class="sxs-lookup"><span data-stu-id="177a9-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="177a9-253">Použití poznámkových bloků Zeppelin s clusterem Sparku v HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="177a9-254">Použití externích balíčků s poznámkovými bloky Jupyter</span><span class="sxs-lookup"><span data-stu-id="177a9-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="177a9-255">Instalace Jupyteru do počítače a připojení ke clusteru HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="177a9-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="177a9-256">Správa prostředků</span><span class="sxs-lookup"><span data-stu-id="177a9-256">Manage resources</span></span>
* [<span data-ttu-id="177a9-257">Správa prostředků v clusteru Apache Spark v Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="177a9-258">Sledování a ladění úloh spuštěných v clusteru Apache Spark v HDInsight</span><span class="sxs-lookup"><span data-stu-id="177a9-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
